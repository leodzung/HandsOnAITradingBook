{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC EDGAR S-1 Filing Downloader\n",
    "\n",
    "**IMPORTANT: Run this notebook on your LOCAL computer, NOT in QuantConnect!**\n",
    "\n",
    "QuantConnect has network restrictions that block SEC EDGAR access. This notebook downloads S-1 filings to your local machine, then you can upload them to QuantConnect if needed.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Reads IPO calendar CSV with ticker symbols\n",
    "2. Searches SEC EDGAR for each company's S-1 filing\n",
    "3. Downloads the S-1 HTML document\n",
    "4. Saves files locally for parsing\n",
    "5. Generates download report\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "pip install pandas requests beautifulsoup4 lxml tqdm\n",
    "```\n",
    "\n",
    "## SEC Requirements\n",
    "\n",
    "The SEC requires you to:\n",
    "- Identify yourself with email in User-Agent header\n",
    "- Limit to 10 requests per second\n",
    "- Not overwhelm their servers\n",
    "\n",
    "**You MUST update your email below before running!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "Current directory: /Users/leole/workspace/HandsOnAITradingBook/IPO Pre-Analysis Day-1 Trading Strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leole/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION - UPDATE THESE!\n",
    "\n",
    "# REQUIRED: Your email address (SEC requirement)\n",
    "YOUR_EMAIL = \"ltd@bu.edu\"  # ⚠️ CHANGE THIS!\n",
    "YOUR_NAME = \"Gnu Led LLC\"      # ⚠️ CHANGE THIS!\n",
    "\n",
    "# Input/Output paths\n",
    "INPUT_CSV = \"data/ipo_calendar.csv\"     # CSV with 'ticker' column\n",
    "OUTPUT_DIR = \"data/s1_filings/\"         # Where to save S-1 files\n",
    "\n",
    "# Download settings\n",
    "DELAY_BETWEEN_REQUESTS = 0.5            # Seconds (SEC allows 10/sec, we use 2/sec)\n",
    "MAX_RETRIES = 3                         # Retry failed downloads\n",
    "TIMEOUT = 30                            # Request timeout (seconds)\n",
    "\n",
    "# Validation\n",
    "if \"your.email@example.com\" in YOUR_EMAIL:\n",
    "    print(\"❌ ERROR: Please update YOUR_EMAIL with your actual email!\")\n",
    "    print(\"The SEC requires identification in the User-Agent header.\")\n",
    "    print(\"This is a legal requirement, not optional.\")\n",
    "else:\n",
    "    print(\"✓ Configuration valid\")\n",
    "    print(f\"User-Agent: {YOUR_NAME} {YOUR_EMAIL}\")\n",
    "    print(f\"Input: {INPUT_CSV}\")\n",
    "    print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    \"\"\"Generate SEC-compliant headers.\"\"\"\n",
    "    return {\n",
    "        'User-Agent': f\"{YOUR_NAME} {YOUR_EMAIL}\",\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Host': 'www.sec.gov'\n",
    "    }\n",
    "\n",
    "def make_request(url, retries=MAX_RETRIES):\n",
    "    \"\"\"Make HTTP request with retry logic.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=get_headers(), timeout=TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise e\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    return None\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik_from_ticker(ticker):\n",
    "    \"\"\"\n",
    "    Get CIK (Central Index Key) from ticker symbol.\n",
    "    \n",
    "    SEC uses CIK numbers instead of ticker symbols internally.\n",
    "    \"\"\"\n",
    "    search_url = f\"https://www.sec.gov/cgi-bin/browse-edgar?company={ticker}&action=getcompany\"\n",
    "    \n",
    "    try:\n",
    "        response = make_request(search_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Look for CIK in the company name span\n",
    "        company_info = soup.find('span', {'class': 'companyName'})\n",
    "        \n",
    "        if company_info:\n",
    "            text = company_info.get_text()\n",
    "            if 'CIK#:' in text:\n",
    "                # Extract CIK like \"CIK#: 0001234567\"\n",
    "                cik = text.split('CIK#:')[1].split()[0].strip()\n",
    "                return cik\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error getting CIK for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "test_cik = get_cik_from_ticker(\"AAPL\")\n",
    "print(f\"✓ CIK lookup working (AAPL CIK: {test_cik})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_s1_url(ticker, cik=None):\n",
    "    \"\"\"\n",
    "    Find the URL for the main S-1 filing document.\n",
    "    \n",
    "    Returns:\n",
    "        URL of S-1 HTML document, or None if not found\n",
    "    \"\"\"\n",
    "    # Get CIK if not provided\n",
    "    if not cik:\n",
    "        cik = get_cik_from_ticker(ticker)\n",
    "        if not cik:\n",
    "            return None\n",
    "    \n",
    "    # Search for S-1 filings\n",
    "    search_url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type=S-1&dateb=&owner=exclude&count=10\"\n",
    "    \n",
    "    try:\n",
    "        response = make_request(search_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find filings table\n",
    "        filings_table = soup.find('table', {'class': 'tableFile2'})\n",
    "        \n",
    "        if not filings_table:\n",
    "            return None\n",
    "        \n",
    "        # Get first filing (most recent)\n",
    "        rows = filings_table.find_all('tr')[1:]  # Skip header\n",
    "        \n",
    "        if not rows:\n",
    "            return None\n",
    "        \n",
    "        # Find documents link in first row\n",
    "        first_row = rows[0]\n",
    "        docs_button = first_row.find('a', {'id': 'documentsbutton'})\n",
    "        \n",
    "        if not docs_button:\n",
    "            return None\n",
    "        \n",
    "        # Go to documents page\n",
    "        docs_url = 'https://www.sec.gov' + docs_button['href']\n",
    "        response = make_request(docs_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find document table\n",
    "        doc_table = soup.find('table', {'class': 'tableFile'})\n",
    "        \n",
    "        if not doc_table:\n",
    "            return None\n",
    "        \n",
    "        # Find main S-1 document\n",
    "        for row in doc_table.find_all('tr')[1:]:\n",
    "            cols = row.find_all('td')\n",
    "            \n",
    "            if len(cols) >= 4:\n",
    "                doc_type = cols[3].get_text().strip()\n",
    "                \n",
    "                # Look for S-1 or S-1/A document\n",
    "                if 'S-1' in doc_type:\n",
    "                    doc_link = cols[2].find('a')\n",
    "                    \n",
    "                    if doc_link and doc_link.get('href'):\n",
    "                        doc_url = 'https://www.sec.gov' + doc_link['href']\n",
    "                        return doc_url\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error finding S-1 for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "test_url = find_s1_url(\"RDDT\")\n",
    "if test_url:\n",
    "    print(f\"✓ S-1 URL finder working\")\n",
    "    print(f\"  Example URL: {test_url[:80]}...\")\n",
    "else:\n",
    "    print(\"⚠️  S-1 finder test failed (RDDT might not have S-1, or network issue)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s1(ticker, output_dir, cik=None):\n",
    "    \"\"\"\n",
    "    Download S-1 filing and save as HTML.\n",
    "    \n",
    "    Returns:\n",
    "        dict with status, filepath, and any error message\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'ticker': ticker,\n",
    "        'success': False,\n",
    "        'filepath': None,\n",
    "        'cik': cik,\n",
    "        'error': None,\n",
    "        'url': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if already downloaded\n",
    "        output_path = os.path.join(output_dir, f\"{ticker}_s1.html\")\n",
    "        if os.path.exists(output_path):\n",
    "            result['success'] = True\n",
    "            result['filepath'] = output_path\n",
    "            result['error'] = 'Already exists (skipped)'\n",
    "            return result\n",
    "        \n",
    "        # Find S-1 URL\n",
    "        url = find_s1_url(ticker, cik)\n",
    "        \n",
    "        if not url:\n",
    "            result['error'] = 'S-1 filing not found'\n",
    "            return result\n",
    "        \n",
    "        result['url'] = url\n",
    "        \n",
    "        # Download document\n",
    "        response = make_request(url)\n",
    "        \n",
    "        # Save to file\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = os.path.getsize(output_path) / 1024  # KB\n",
    "        \n",
    "        result['success'] = True\n",
    "        result['filepath'] = output_path\n",
    "        result['file_size_kb'] = file_size\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        return result\n",
    "\n",
    "print(\"✓ Download function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IPO Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IPO calendar\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    print(f\"❌ Input file not found: {INPUT_CSV}\")\n",
    "    print(\"\\nCreating sample IPO calendar...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_ipos = [\n",
    "        {'ticker': 'ARM', 'company': 'Arm Holdings', 'ipo_date': '2023-09-14', 'sector': 'Technology'},\n",
    "        {'ticker': 'RDDT', 'company': 'Reddit Inc', 'ipo_date': '2024-03-21', 'sector': 'Technology'},\n",
    "        {'ticker': 'CART', 'company': 'Instacart', 'ipo_date': '2023-09-19', 'sector': 'Technology'},\n",
    "        {'ticker': 'BIRK', 'company': 'Birkenstock', 'ipo_date': '2023-10-11', 'sector': 'Consumer'},\n",
    "        {'ticker': 'KVUE', 'company': 'Kenvue Inc', 'ipo_date': '2023-05-04', 'sector': 'Healthcare'},\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(sample_ipos)\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    df.to_csv(INPUT_CSV, index=False)\n",
    "    print(f\"✓ Created sample calendar with {len(df)} IPOs\")\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    print(f\"✓ Loaded {len(df)} IPOs from {INPUT_CSV}\")\n",
    "\n",
    "# Validate\n",
    "if 'ticker' not in df.columns:\n",
    "    print(\"❌ ERROR: CSV must have 'ticker' column\")\n",
    "else:\n",
    "    print(f\"\\nIPOs to download:\")\n",
    "    print(df[['ticker', 'company'] if 'company' in df.columns else ['ticker']].head(10))\n",
    "    \n",
    "    if len(df) > 10:\n",
    "        print(f\"... and {len(df) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download S-1 Filings\n",
    "\n",
    "This will:\n",
    "1. Loop through all tickers\n",
    "2. Search SEC EDGAR for S-1 filing\n",
    "3. Download and save HTML\n",
    "4. Show progress bar\n",
    "5. Generate report\n",
    "\n",
    "**Estimated time:** ~30 seconds per IPO (includes SEC rate limiting)\n",
    "- 10 IPOs: ~5 minutes\n",
    "- 50 IPOs: ~25 minutes\n",
    "- 100 IPOs: ~50 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Limit number for testing\n",
    "LIMIT = None  # Set to 5 for testing, None for all\n",
    "\n",
    "tickers = df['ticker'].tolist()\n",
    "if LIMIT:\n",
    "    tickers = tickers[:LIMIT]\n",
    "    print(f\"⚠️  Limited to first {LIMIT} tickers for testing\")\n",
    "\n",
    "print(f\"\\nDownloading S-1 filings for {len(tickers)} IPOs...\")\n",
    "print(f\"Estimated time: ~{len(tickers) * 30 / 60:.0f} minutes\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Download each filing\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for ticker in tqdm(tickers, desc=\"Downloading\"):\n",
    "    result = download_s1(ticker, OUTPUT_DIR)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Show status in progress bar\n",
    "    status = \"✓\" if result['success'] else \"✗\"\n",
    "    tqdm.write(f\"{status} {ticker:6s} - {result.get('error', 'Downloaded')}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Create results dataframe\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Download Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Average: {elapsed/len(tickers):.1f} seconds per IPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total = len(df_results)\n",
    "successful = df_results['success'].sum()\n",
    "failed = total - successful\n",
    "success_rate = successful / total * 100\n",
    "\n",
    "print(f\"\\n📊 Summary Statistics\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Total IPOs:        {total}\")\n",
    "print(f\"Successful:        {successful} ({success_rate:.1f}%)\")\n",
    "print(f\"Failed:            {failed} ({100-success_rate:.1f}%)\")\n",
    "\n",
    "# File sizes\n",
    "if 'file_size_kb' in df_results.columns:\n",
    "    downloaded = df_results[df_results['file_size_kb'].notna()]\n",
    "    if len(downloaded) > 0:\n",
    "        print(f\"\\nAverage file size: {downloaded['file_size_kb'].mean():.0f} KB\")\n",
    "        print(f\"Total downloaded:  {downloaded['file_size_kb'].sum()/1024:.1f} MB\")\n",
    "\n",
    "# Error breakdown\n",
    "if failed > 0:\n",
    "    print(f\"\\n❌ Failed Downloads ({failed}):\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    failed_df = df_results[~df_results['success']]\n",
    "    error_counts = failed_df['error'].value_counts()\n",
    "    \n",
    "    for error, count in error_counts.items():\n",
    "        print(f\"  {error}: {count}\")\n",
    "    \n",
    "    print(\"\\nFailed tickers:\")\n",
    "    for _, row in failed_df.iterrows():\n",
    "        print(f\"  - {row['ticker']:6s}: {row['error']}\")\n",
    "\n",
    "# Success examples\n",
    "if successful > 0:\n",
    "    print(f\"\\n✓ Successfully Downloaded ({min(5, successful)} examples):\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    success_df = df_results[df_results['success']]\n",
    "    for _, row in success_df.head(5).iterrows():\n",
    "        size = f\"{row['file_size_kb']:.0f} KB\" if 'file_size_kb' in row and pd.notna(row['file_size_kb']) else \"N/A\"\n",
    "        print(f\"  {row['ticker']:6s} - {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Download Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed log\n",
    "log_file = os.path.join(OUTPUT_DIR, '_download_log.csv')\n",
    "df_results.to_csv(log_file, index=False)\n",
    "\n",
    "print(f\"\\n📄 Download log saved: {log_file}\")\n",
    "\n",
    "# Save summary report\n",
    "summary_file = os.path.join(OUTPUT_DIR, '_download_summary.txt')\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"SEC EDGAR S-1 Download Report\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Total IPOs: {total}\\n\")\n",
    "    f.write(f\"Successful: {successful} ({success_rate:.1f}%)\\n\")\n",
    "    f.write(f\"Failed: {failed}\\n\")\n",
    "    f.write(f\"Time: {elapsed/60:.1f} minutes\\n\\n\")\n",
    "    \n",
    "    if failed > 0:\n",
    "        f.write(\"Failed Tickers:\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for _, row in df_results[~df_results['success']].iterrows():\n",
    "            f.write(f\"{row['ticker']}: {row['error']}\\n\")\n",
    "\n",
    "print(f\"📄 Summary report saved: {summary_file}\")\n",
    "\n",
    "print(f\"\\n✓ All files saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all downloaded files\n",
    "s1_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('_s1.html')]\n",
    "\n",
    "print(f\"\\n📁 Files in {OUTPUT_DIR}:\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"Total S-1 HTML files: {len(s1_files)}\")\n",
    "\n",
    "if len(s1_files) > 0:\n",
    "    print(f\"\\nFirst 10 files:\")\n",
    "    for filename in sorted(s1_files)[:10]:\n",
    "        filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "        size = os.path.getsize(filepath) / 1024  # KB\n",
    "        print(f\"  {filename:30s} {size:8.0f} KB\")\n",
    "    \n",
    "    if len(s1_files) > 10:\n",
    "        print(f\"  ... and {len(s1_files) - 10} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Sample S-1 (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first downloaded S-1\n",
    "if s1_files:\n",
    "    sample_file = os.path.join(OUTPUT_DIR, s1_files[0])\n",
    "    \n",
    "    print(f\"\\n📄 Preview of {s1_files[0]}:\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Show first 1000 characters\n",
    "    preview = text[:1000].strip()\n",
    "    print(preview)\n",
    "    print(\"\\n...\")\n",
    "    print(f\"\\n✓ File is valid HTML with {len(text)} characters of text\")\n",
    "else:\n",
    "    print(\"No S-1 files to preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### What to Do Now:\n",
    "\n",
    "1. **Review failed downloads:**\n",
    "   - Check `_download_log.csv` for errors\n",
    "   - Manually download missing S-1s from SEC EDGAR\n",
    "   - Some companies may not have S-1s (foreign issuers use F-1)\n",
    "\n",
    "2. **Parse fundamentals:**\n",
    "   - Go to `data_collection.ipynb` Step 3\n",
    "   - Extract financial data from S-1 HTML files\n",
    "   - Use manual entry template for accuracy\n",
    "\n",
    "3. **Upload to QuantConnect (if needed):**\n",
    "   - Zip the `data/s1_filings/` folder\n",
    "   - Upload to your QuantConnect project\n",
    "   - Reference in data_collection.ipynb\n",
    "\n",
    "### Manual Download for Failed IPOs:\n",
    "\n",
    "For any failed downloads:\n",
    "1. Visit: `https://www.sec.gov/cgi-bin/browse-edgar?company=TICKER&type=S-1`\n",
    "2. Find most recent S-1 or S-1/A filing\n",
    "3. Click \"Documents\" button\n",
    "4. Download the main .htm file\n",
    "5. Save as `TICKER_s1.html` in the output directory\n",
    "\n",
    "### Success Criteria:\n",
    "\n",
    "- ✓ 80%+ success rate is excellent\n",
    "- ✓ Files should be 100-500 KB each (typical S-1 size)\n",
    "- ✓ Can open HTML files in browser and see financial tables\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "**No S-1 found:**\n",
    "- Company may have filed F-1 (foreign issuers)\n",
    "- Check if company went public via SPAC (different filings)\n",
    "- Some direct listings don't have S-1s\n",
    "\n",
    "**Rate limit errors:**\n",
    "- Increase DELAY_BETWEEN_REQUESTS to 1.0 seconds\n",
    "- Run during off-peak hours\n",
    "\n",
    "**Connection errors:**\n",
    "- Check internet connection\n",
    "- SEC EDGAR may be down (rare)\n",
    "- Try again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 S-1 DOWNLOAD COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n✓ Downloaded {successful}/{total} S-1 filings\")\n",
    "print(f\"✓ Saved to: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "print(f\"✓ Log file: {log_file}\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(f\"\\n⚠️  {failed} downloads failed - see log for details\")\n",
    "    print(f\"You can manually download these from SEC EDGAR\")\n",
    "\n",
    "print(f\"\\n📋 Next: Extract fundamentals from S-1 files\")\n",
    "print(f\"   → Open data_collection.ipynb Step 3\")\n",
    "print(f\"   → Or manually enter data into CSV template\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
