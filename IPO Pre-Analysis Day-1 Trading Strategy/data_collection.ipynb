{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPO Historical Data Collection\n",
    "\n",
    "This notebook walks you through collecting historical IPO data for model training.\n",
    "\n",
    "**Goal:** Collect 100-300 IPOs from 2023-2024 to start\n",
    "\n",
    "**Sections:**\n",
    "1. IPO Calendar Data (ticker, listing date, offer price)\n",
    "2. S-1 Fundamental Data (revenue, margins, etc.)\n",
    "3. Price Performance Data (30-day returns)\n",
    "4. Market Conditions Data (VIX, SPY returns)\n",
    "5. Sentiment Data (news, FinBERT)\n",
    "6. Data Validation & Merge\n",
    "\n",
    "**Estimated Time:** 4-6 hours for 100 IPOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# QuantConnect for price data\n",
    "qb = QuantBook()\n",
    "\n",
    "# Create data directory\n",
    "import os\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('data/s1_filings', exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Collect IPO Calendar Data\n",
    "\n",
    "We'll use multiple sources and merge them.\n",
    "\n",
    "### Option A: Manual Collection (Easiest, Recommended for Start)\n",
    "\n",
    "1. Go to: https://www.renaissancecapital.com/IPO-Center/IPO-Performance\n",
    "2. Filter: Year 2023-2024\n",
    "3. Export to Excel or copy table\n",
    "4. Save as `data/raw/ipo_calendar_manual.csv`\n",
    "\n",
    "**Required Columns:**\n",
    "- Ticker\n",
    "- Company Name  \n",
    "- IPO Date\n",
    "- Offer Price\n",
    "- Shares Offered\n",
    "- First Day Close\n",
    "- Underwriters\n",
    "- Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manually collected data\n",
    "# After you've downloaded from Renaissance Capital, run this:\n",
    "\n",
    "# df_calendar = pd.read_csv('data/raw/ipo_calendar_manual.csv')\n",
    "\n",
    "# For now, create a starter list of major 2023-2024 IPOs\n",
    "starter_ipos = [\n",
    "    {'ticker': 'ARM', 'company': 'Arm Holdings', 'ipo_date': '2023-09-14', 'offer_price': 51.00, 'sector': 'Technology'},\n",
    "    {'ticker': 'BIRK', 'company': 'Birkenstock', 'ipo_date': '2023-10-11', 'offer_price': 46.00, 'sector': 'Consumer'},\n",
    "    {'ticker': 'KVUE', 'company': 'Kenvue Inc', 'ipo_date': '2023-05-04', 'offer_price': 22.00, 'sector': 'Healthcare'},\n",
    "    {'ticker': 'FYBR', 'company': 'Frontier Communications', 'ipo_date': '2023-05-04', 'offer_price': 25.00, 'sector': 'Telecom'},\n",
    "    {'ticker': 'CRS', 'company': 'Carpenter Technology', 'ipo_date': '2023-05-11', 'offer_price': 15.00, 'sector': 'Materials'},\n",
    "    {'ticker': 'RXST', 'company': 'RxSight Inc', 'ipo_date': '2023-05-17', 'offer_price': 15.00, 'sector': 'Healthcare'},\n",
    "    {'ticker': 'CART', 'company': 'Maplebear Inc (Instacart)', 'ipo_date': '2023-09-19', 'offer_price': 30.00, 'sector': 'Technology'},\n",
    "    {'ticker': 'VLD', 'company': 'Valdo Inc', 'ipo_date': '2023-10-05', 'offer_price': 17.00, 'sector': 'Technology'},\n",
    "    {'ticker': 'RDDT', 'company': 'Reddit Inc', 'ipo_date': '2024-03-21', 'offer_price': 34.00, 'sector': 'Technology'},\n",
    "    {'ticker': 'ASND', 'company': 'Ascendis Pharma', 'ipo_date': '2024-02-01', 'offer_price': 12.00, 'sector': 'Healthcare'},\n",
    "    # Add more as you collect them\n",
    "]\n",
    "\n",
    "df_calendar = pd.DataFrame(starter_ipos)\n",
    "df_calendar['ipo_date'] = pd.to_datetime(df_calendar['ipo_date'])\n",
    "\n",
    "print(f\"Loaded {len(df_calendar)} IPOs\")\n",
    "print(\"\\nSample:\")\n",
    "df_calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Automated Scraping (Advanced)\n",
    "\n",
    "Use this to scrape from public sources. Note: May require maintenance if sites change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nasdaq_ipo_calendar(year=2024):\n",
    "    \"\"\"\n",
    "    Scrape Nasdaq IPO calendar (example - may need updates)\n",
    "    \"\"\"\n",
    "    url = f\"https://www.nasdaq.com/market-activity/ipos?year={year}\"\n",
    "    \n",
    "    # Add headers to avoid blocking\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # This is site-specific - inspect the page to find correct selectors\n",
    "        # tables = soup.find_all('table')\n",
    "        \n",
    "        # For now, return empty (you'll need to customize based on site structure)\n",
    "        print(f\"Successfully fetched page for {year}\")\n",
    "        print(\"Note: You'll need to inspect the HTML and update parsing logic\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Try scraping (likely needs customization)\n",
    "# df_scraped = scrape_nasdaq_ipo_calendar(2024)\n",
    "print(\"Scraping function defined. Customize based on your target site.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download S-1 Filings from SEC EDGAR\n",
    "\n",
    "For each IPO, we need the S-1 filing to extract fundamentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sec_filing_url(ticker, filing_type='S-1'):\n",
    "    \"\"\"\n",
    "    Find SEC filing URL for a given ticker.\n",
    "    \n",
    "    Uses SEC EDGAR search API.\n",
    "    \"\"\"\n",
    "    # SEC EDGAR CIK lookup\n",
    "    search_url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&company={ticker}&type={filing_type}&dateb=&owner=exclude&count=10\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'YourName your.email@example.com',  # SEC requires identification\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Host': 'www.sec.gov'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        time.sleep(0.5)  # Be polite to SEC servers\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find documents table\n",
    "        filing_table = soup.find('table', {'class': 'tableFile2'})\n",
    "        \n",
    "        if filing_table:\n",
    "            # Get first S-1 filing\n",
    "            rows = filing_table.find_all('tr')[1:]  # Skip header\n",
    "            \n",
    "            if rows:\n",
    "                first_filing = rows[0]\n",
    "                doc_link = first_filing.find('a', {'id': 'documentsbutton'})\n",
    "                \n",
    "                if doc_link:\n",
    "                    doc_url = 'https://www.sec.gov' + doc_link['href']\n",
    "                    return doc_url\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_s1_filing(ticker, save_dir='data/s1_filings'):\n",
    "    \"\"\"\n",
    "    Download S-1 filing HTML for a ticker.\n",
    "    \"\"\"\n",
    "    url = get_sec_filing_url(ticker)\n",
    "    \n",
    "    if not url:\n",
    "        print(f\"No S-1 found for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'YourName your.email@example.com'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Save HTML\n",
    "        filepath = f\"{save_dir}/{ticker}_s1.html\"\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        print(f\"✓ Downloaded S-1 for {ticker}\")\n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with one IPO\n",
    "test_ticker = df_calendar.iloc[0]['ticker']\n",
    "print(f\"Testing S-1 download for {test_ticker}...\")\n",
    "filepath = download_s1_filing(test_ticker)\n",
    "print(\"\\nNote: Uncomment the line above to actually download.\")\n",
    "print(\"SEC requires you identify yourself with email in User-Agent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download S-1 for all IPOs in calendar\n",
    "def download_all_s1_filings(df_calendar, limit=None):\n",
    "    \"\"\"\n",
    "    Download S-1 filings for all IPOs.\n",
    "    \n",
    "    Args:\n",
    "        df_calendar: DataFrame with 'ticker' column\n",
    "        limit: Max number to download (None = all)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    tickers = df_calendar['ticker'].tolist()\n",
    "    \n",
    "    if limit:\n",
    "        tickers = tickers[:limit]\n",
    "    \n",
    "    print(f\"Downloading S-1 filings for {len(tickers)} IPOs...\")\n",
    "    print(\"This may take 5-10 minutes.\\n\")\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"[{i}/{len(tickers)}] {ticker}...\", end=' ')\n",
    "        filepath = download_s1_filing(ticker)\n",
    "        \n",
    "        results.append({\n",
    "            'ticker': ticker,\n",
    "            's1_downloaded': filepath is not None,\n",
    "            's1_path': filepath\n",
    "        })\n",
    "        \n",
    "        time.sleep(1)  # Be polite to SEC servers\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    success_rate = df_results['s1_downloaded'].mean()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Download complete!\")\n",
    "    print(f\"Success rate: {success_rate:.1%}\")\n",
    "    print(f\"Successfully downloaded: {df_results['s1_downloaded'].sum()}/{len(df_results)}\")\n",
    "    \n",
    "    # Save results\n",
    "    df_results.to_csv('data/raw/s1_download_results.csv', index=False)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Uncomment to run (downloads take time)\n",
    "# df_s1_results = download_all_s1_filings(df_calendar, limit=5)  # Start with 5\n",
    "print(\"Ready to download S-1 filings.\")\n",
    "print(\"Uncomment the line above and run when ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Fundamental Data from S-1 Filings\n",
    "\n",
    "Parse the S-1 HTML/PDF to extract financial metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_s1_financials(filepath):\n",
    "    \"\"\"\n",
    "    Parse fundamental data from S-1 HTML file.\n",
    "    \n",
    "    This is a simplified version. Real implementation needs:\n",
    "    - Better table detection\n",
    "    - Multiple parsing strategies\n",
    "    - Manual review and correction\n",
    "    \n",
    "    Returns dict with fundamental metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Initialize default values\n",
    "        fundamentals = {\n",
    "            'revenue': None,\n",
    "            'revenue_growth_yoy': None,\n",
    "            'gross_margin': None,\n",
    "            'operating_margin': None,\n",
    "            'net_income': None,\n",
    "            'cash': None,\n",
    "            'debt': None,\n",
    "            'employees': None,\n",
    "            'founded_year': None\n",
    "        }\n",
    "        \n",
    "        # Try to extract revenue (example pattern)\n",
    "        revenue_patterns = [\n",
    "            r'Revenue[\\s\\S]{0,100}?\\$([0-9,]+)',\n",
    "            r'Total revenue[\\s\\S]{0,100}?\\$([0-9,]+)',\n",
    "            r'Net revenue[\\s\\S]{0,100}?\\$([0-9,]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in revenue_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                revenue_str = match.group(1).replace(',', '')\n",
    "                fundamentals['revenue'] = float(revenue_str) * 1000  # Assumes millions\n",
    "                break\n",
    "        \n",
    "        # Extract employees\n",
    "        employee_pattern = r'([0-9,]+)\\s+employees'\n",
    "        match = re.search(employee_pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            fundamentals['employees'] = int(match.group(1).replace(',', ''))\n",
    "        \n",
    "        # Extract founded year\n",
    "        founded_patterns = [\n",
    "            r'founded in ([0-9]{4})',\n",
    "            r'incorporated in ([0-9]{4})',\n",
    "            r'established in ([0-9]{4})'\n",
    "        ]\n",
    "        \n",
    "        for pattern in founded_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                fundamentals['founded_year'] = int(match.group(1))\n",
    "                break\n",
    "        \n",
    "        return fundamentals\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {filepath}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Test parser on one file\n",
    "test_file = 'data/s1_filings/ARM_s1.html'  # Replace with actual downloaded file\n",
    "if os.path.exists(test_file):\n",
    "    parsed = parse_s1_financials(test_file)\n",
    "    print(\"Parsed fundamentals:\")\n",
    "    print(json.dumps(parsed, indent=2))\n",
    "else:\n",
    "    print(f\"Test file not found: {test_file}\")\n",
    "    print(\"Download S-1 filings first, then run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual S-1 Data Entry Template\n",
    "\n",
    "For higher accuracy, manually enter key metrics from each S-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create template for manual data entry\n",
    "manual_template = pd.DataFrame({\n",
    "    'ticker': df_calendar['ticker'],\n",
    "    'revenue': np.nan,\n",
    "    'revenue_previous_year': np.nan,\n",
    "    'gross_profit': np.nan,\n",
    "    'operating_income': np.nan,\n",
    "    'net_income': np.nan,\n",
    "    'cash': np.nan,\n",
    "    'total_debt': np.nan,\n",
    "    'employees': np.nan,\n",
    "    'founded_year': np.nan,\n",
    "    'shares_outstanding': np.nan,\n",
    "    'top5_customer_pct': np.nan,\n",
    "    'notes': ''  # For any special observations\n",
    "})\n",
    "\n",
    "# Save template\n",
    "manual_template.to_csv('data/raw/s1_manual_entry_template.csv', index=False)\n",
    "\n",
    "print(\"Manual entry template created!\")\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"1. Open: data/raw/s1_manual_entry_template.csv\")\n",
    "print(\"2. For each IPO, find the S-1 filing 'Summary Financial Data' table\")\n",
    "print(\"3. Enter values (use most recent fiscal year)\")\n",
    "print(\"4. Save as: data/raw/s1_manual_fundamentals.csv\")\n",
    "print(\"5. Come back and run next cell to load data\")\n",
    "\n",
    "print(\"\\nExample S-1 locations:\")\n",
    "for ticker in df_calendar['ticker'][:3]:\n",
    "    print(f\"- {ticker}: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&company={ticker}&type=S-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manually entered data\n",
    "# After you've filled in the template, run this:\n",
    "\n",
    "manual_file = 'data/raw/s1_manual_fundamentals.csv'\n",
    "\n",
    "if os.path.exists(manual_file):\n",
    "    df_fundamentals = pd.read_csv(manual_file)\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    df_fundamentals['revenue_growth_yoy'] = (\n",
    "        (df_fundamentals['revenue'] - df_fundamentals['revenue_previous_year']) \n",
    "        / df_fundamentals['revenue_previous_year']\n",
    "    )\n",
    "    \n",
    "    df_fundamentals['gross_margin'] = (\n",
    "        df_fundamentals['gross_profit'] / df_fundamentals['revenue']\n",
    "    )\n",
    "    \n",
    "    df_fundamentals['operating_margin'] = (\n",
    "        df_fundamentals['operating_income'] / df_fundamentals['revenue']\n",
    "    )\n",
    "    \n",
    "    df_fundamentals['is_profitable'] = (df_fundamentals['net_income'] > 0).astype(int)\n",
    "    \n",
    "    df_fundamentals['debt_to_equity'] = (\n",
    "        df_fundamentals['total_debt'] / (df_fundamentals['revenue'] * 0.5)  # Rough equity proxy\n",
    "    )\n",
    "    \n",
    "    df_fundamentals['company_age'] = datetime.now().year - df_fundamentals['founded_year']\n",
    "    \n",
    "    print(f\"Loaded fundamentals for {len(df_fundamentals)} IPOs\")\n",
    "    print(f\"\\nCompleteness: {df_fundamentals['revenue'].notna().sum()}/{len(df_fundamentals)} have revenue data\")\n",
    "    print(\"\\nSample:\")\n",
    "    display(df_fundamentals[['ticker', 'revenue', 'revenue_growth_yoy', 'gross_margin', 'is_profitable']].head())\n",
    "    \n",
    "else:\n",
    "    print(f\"Manual data file not found: {manual_file}\")\n",
    "    print(\"Please fill in the template first.\")\n",
    "    df_fundamentals = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Collect Price Performance Data\n",
    "\n",
    "Get historical prices using QuantConnect to calculate 30-day returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ipo_performance(ticker, ipo_date, days=30):\n",
    "    \"\"\"\n",
    "    Get IPO price performance using QuantConnect.\n",
    "    \n",
    "    Returns:\n",
    "        dict with first_day_close, day30_close, return_30d, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add equity\n",
    "        equity = qb.add_equity(ticker)\n",
    "        \n",
    "        # Get history from IPO date to 30 days later\n",
    "        end_date = ipo_date + timedelta(days=days+10)  # Buffer for weekends\n",
    "        history = qb.history(equity.symbol, ipo_date, end_date, Resolution.DAILY)\n",
    "        \n",
    "        if history.empty:\n",
    "            return None\n",
    "        \n",
    "        # Get first trading day close (IPO day)\n",
    "        first_close = history['close'].iloc[0]\n",
    "        \n",
    "        # Get close 30 trading days later\n",
    "        if len(history) >= days:\n",
    "            day30_close = history['close'].iloc[min(days-1, len(history)-1)]\n",
    "        else:\n",
    "            # If not enough data, use last available\n",
    "            day30_close = history['close'].iloc[-1]\n",
    "            print(f\"  Warning: Only {len(history)} days of data for {ticker}\")\n",
    "        \n",
    "        # Calculate returns\n",
    "        return_30d = (day30_close - first_close) / first_close\n",
    "        \n",
    "        # Binary target: success if return > 10%\n",
    "        success = 1 if return_30d > 0.10 else 0\n",
    "        \n",
    "        return {\n",
    "            'ticker': ticker,\n",
    "            'first_day_close': first_close,\n",
    "            'day30_close': day30_close,\n",
    "            'return_30d': return_30d,\n",
    "            'success': success,\n",
    "            'days_of_data': len(history)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error getting data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with one IPO\n",
    "test_ticker = df_calendar.iloc[0]['ticker']\n",
    "test_date = df_calendar.iloc[0]['ipo_date']\n",
    "\n",
    "print(f\"Testing price data for {test_ticker} (IPO: {test_date.date()})...\")\n",
    "result = get_ipo_performance(test_ticker, test_date)\n",
    "\n",
    "if result:\n",
    "    print(\"\\nResult:\")\n",
    "    print(json.dumps({k: f\"{v:.2%}\" if 'return' in k else v for k, v in result.items()}, indent=2))\n",
    "else:\n",
    "    print(\"Could not retrieve data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance data for all IPOs\n",
    "def get_all_ipo_performance(df_calendar):\n",
    "    \"\"\"\n",
    "    Get 30-day performance for all IPOs in calendar.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Collecting performance data for {len(df_calendar)} IPOs...\\n\")\n",
    "    \n",
    "    for i, row in df_calendar.iterrows():\n",
    "        ticker = row['ticker']\n",
    "        ipo_date = pd.to_datetime(row['ipo_date'])\n",
    "        \n",
    "        print(f\"[{i+1}/{len(df_calendar)}] {ticker}...\", end=' ')\n",
    "        \n",
    "        result = get_ipo_performance(ticker, ipo_date)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"✓ Return: {result['return_30d']:.1%}\")\n",
    "            results.append(result)\n",
    "        else:\n",
    "            print(\"✗ No data\")\n",
    "        \n",
    "        time.sleep(0.2)  # Small delay\n",
    "    \n",
    "    df_performance = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Performance data collected!\")\n",
    "    print(f\"Success rate: {len(df_performance)}/{len(df_calendar)}\")\n",
    "    print(f\"Average 30-day return: {df_performance['return_30d'].mean():.1%}\")\n",
    "    print(f\"Success rate (>10%): {df_performance['success'].mean():.1%}\")\n",
    "    \n",
    "    # Save\n",
    "    df_performance.to_csv('data/raw/ipo_performance.csv', index=False)\n",
    "    \n",
    "    return df_performance\n",
    "\n",
    "# Run collection\n",
    "df_performance = get_all_ipo_performance(df_calendar)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nPerformance Distribution:\")\n",
    "print(df_performance['return_30d'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Collect Market Conditions Data\n",
    "\n",
    "For each IPO date, get VIX, SPY returns, sector ETF returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_conditions(ipo_date, sector_etf='QQQ'):\n",
    "    \"\"\"\n",
    "    Get market conditions on IPO date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # VIX on IPO date\n",
    "        vix = qb.add_index(\"VIX\")\n",
    "        vix_history = qb.history(vix.symbol, ipo_date - timedelta(5), ipo_date + timedelta(1), Resolution.DAILY)\n",
    "        vix_level = vix_history['close'].iloc[-1] if not vix_history.empty else 15.0\n",
    "        \n",
    "        # SPY 30-day return\n",
    "        spy = qb.add_equity(\"SPY\")\n",
    "        spy_history = qb.history(spy.symbol, ipo_date - timedelta(35), ipo_date, Resolution.DAILY)\n",
    "        \n",
    "        if len(spy_history) >= 20:\n",
    "            spy_return_30d = (spy_history['close'].iloc[-1] / spy_history['close'].iloc[0] - 1)\n",
    "        else:\n",
    "            spy_return_30d = 0.02  # Default\n",
    "        \n",
    "        # Sector ETF 30-day return\n",
    "        sector = qb.add_equity(sector_etf)\n",
    "        sector_history = qb.history(sector.symbol, ipo_date - timedelta(35), ipo_date, Resolution.DAILY)\n",
    "        \n",
    "        if len(sector_history) >= 20:\n",
    "            sector_return_30d = (sector_history['close'].iloc[-1] / sector_history['close'].iloc[0] - 1)\n",
    "        else:\n",
    "            sector_return_30d = spy_return_30d  # Fall back to SPY\n",
    "        \n",
    "        return {\n",
    "            'vix': vix_level,\n",
    "            'spy_return_30d': spy_return_30d * 100,  # Convert to %\n",
    "            'sector_return_30d': sector_return_30d * 100,\n",
    "            'ipo_market_temp': spy_return_30d * 100  # Proxy\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Collect for all IPOs\n",
    "market_data = []\n",
    "\n",
    "print(f\"Collecting market conditions for {len(df_calendar)} IPOs...\\n\")\n",
    "\n",
    "for i, row in df_calendar.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    ipo_date = pd.to_datetime(row['ipo_date'])\n",
    "    sector_etf = 'QQQ' if row['sector'] == 'Technology' else 'SPY'\n",
    "    \n",
    "    print(f\"[{i+1}/{len(df_calendar)}] {ticker}...\", end=' ')\n",
    "    \n",
    "    conditions = get_market_conditions(ipo_date, sector_etf)\n",
    "    \n",
    "    if conditions:\n",
    "        conditions['ticker'] = ticker\n",
    "        conditions['ipos_same_week'] = 2  # Default (would need to count from calendar)\n",
    "        market_data.append(conditions)\n",
    "        print(f\"✓ VIX: {conditions['vix']:.1f}\")\n",
    "    else:\n",
    "        print(\"✗\")\n",
    "\n",
    "df_market = pd.DataFrame(market_data)\n",
    "df_market.to_csv('data/raw/ipo_market_conditions.csv', index=False)\n",
    "\n",
    "print(f\"\\nMarket conditions collected!\")\n",
    "print(f\"Average VIX: {df_market['vix'].mean():.1f}\")\n",
    "print(f\"Average SPY return: {df_market['spy_return_30d'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Collect Sentiment Data (FinBERT)\n",
    "\n",
    "Analyze news sentiment before each IPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers tensorflow\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Loading FinBERT model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = TFBertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Get sentiment score using FinBERT.\n",
    "    \n",
    "    Returns:\n",
    "        Score from -1 (negative) to +1 (positive)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(inputs)\n",
    "    probs = tf.nn.softmax(outputs.logits, axis=-1).numpy()[0]\n",
    "    \n",
    "    # FinBERT classes: [negative, neutral, positive]\n",
    "    sentiment_score = probs[2] - probs[0]  # -1 to +1\n",
    "    \n",
    "    return sentiment_score\n",
    "\n",
    "# Test\n",
    "test_text = \"The company reported strong revenue growth and positive outlook for the IPO.\"\n",
    "score = analyze_sentiment(test_text)\n",
    "print(f\"\\nTest sentiment: {score:.3f}\")\n",
    "print(\"Positive score = bullish, negative score = bearish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentiment data, you need news articles\n",
    "# Option 1: Manual - Google \"[company] IPO\" and copy headlines\n",
    "# Option 2: Use news API (e.g., NewsAPI, AlphaSense)\n",
    "\n",
    "# Example: Manual sentiment entry\n",
    "manual_sentiment = []\n",
    "\n",
    "for ticker in df_calendar['ticker']:\n",
    "    # You would:\n",
    "    # 1. Google \"[company] IPO news\"\n",
    "    # 2. Collect 5-10 headlines from month before IPO\n",
    "    # 3. Run through FinBERT\n",
    "    # 4. Average the scores\n",
    "    \n",
    "    # For now, use placeholder\n",
    "    manual_sentiment.append({\n",
    "        'ticker': ticker,\n",
    "        'finbert_score': 0.0,  # TODO: Calculate from news\n",
    "        'news_volume': 0,      # TODO: Count articles\n",
    "        'sentiment_velocity': 0.0,\n",
    "        'social_buzz': 0,\n",
    "        'google_trends': 0\n",
    "    })\n",
    "\n",
    "df_sentiment = pd.DataFrame(manual_sentiment)\n",
    "df_sentiment.to_csv('data/raw/ipo_sentiment.csv', index=False)\n",
    "\n",
    "print(\"Sentiment data template created.\")\n",
    "print(\"\\nTODO: Fill in actual sentiment scores from news analysis.\")\n",
    "print(\"For now, we'll use neutral (0.0) as placeholder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Merge All Data & Create Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "df_calendar = pd.read_csv('data/raw/ipo_calendar_manual.csv') if os.path.exists('data/raw/ipo_calendar_manual.csv') else df_calendar\n",
    "\n",
    "# These you should have generated\n",
    "files_to_load = [\n",
    "    ('fundamentals', 'data/raw/s1_manual_fundamentals.csv'),\n",
    "    ('performance', 'data/raw/ipo_performance.csv'),\n",
    "    ('market', 'data/raw/ipo_market_conditions.csv'),\n",
    "    ('sentiment', 'data/raw/ipo_sentiment.csv')\n",
    "]\n",
    "\n",
    "loaded_data = {}\n",
    "\n",
    "for name, filepath in files_to_load:\n",
    "    if os.path.exists(filepath):\n",
    "        loaded_data[name] = pd.read_csv(filepath)\n",
    "        print(f\"✓ Loaded {name}: {len(loaded_data[name])} rows\")\n",
    "    else:\n",
    "        print(f\"✗ Missing {name}: {filepath}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "\n",
    "# Merge all datasets\n",
    "if len(loaded_data) == 4:\n",
    "    df_final = df_calendar.copy()\n",
    "    \n",
    "    for name, df in loaded_data.items():\n",
    "        df_final = df_final.merge(df, on='ticker', how='left')\n",
    "    \n",
    "    # Drop rows without target variable\n",
    "    df_final = df_final.dropna(subset=['success'])\n",
    "    \n",
    "    # Add deal characteristics (if not already present)\n",
    "    if 'price_vs_range' not in df_final.columns:\n",
    "        # Would calculate from price range data\n",
    "        df_final['price_vs_range'] = 0  # Placeholder\n",
    "        df_final['float_pct'] = 20  # Placeholder\n",
    "        df_final['underwriter_tier'] = 0  # Placeholder\n",
    "        df_final['lockup_days'] = 180  # Default\n",
    "        df_final['greenshoe_pct'] = 15  # Default\n",
    "        df_final['proceeds_for_growth'] = 0.5  # Placeholder\n",
    "        df_final['subscription_level'] = 0  # Placeholder\n",
    "    \n",
    "    # Save final dataset\n",
    "    df_final.to_csv('data/historical_ipos.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nFinal dataset created!\")\n",
    "    print(f\"Total IPOs: {len(df_final)}\")\n",
    "    print(f\"Total features: {len(df_final.columns)}\")\n",
    "    print(f\"Success rate: {df_final['success'].mean():.1%}\")\n",
    "    \n",
    "    print(f\"\\nSaved to: data/historical_ipos.csv\")\n",
    "    print(\"\\nReady for model training! Go to research.ipynb\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    display(df_final[['ticker', 'ipo_date', 'return_30d', 'success', 'revenue', 'vix']].head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"\\nCannot merge - missing datasets.\")\n",
    "    print(\"Complete Steps 1-6 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "✅ Collected IPO calendar data  \n",
    "✅ Downloaded S-1 filings  \n",
    "✅ Extracted fundamental metrics  \n",
    "✅ Calculated 30-day returns (target variable)  \n",
    "✅ Collected market conditions  \n",
    "✅ Analyzed sentiment with FinBERT  \n",
    "✅ Created final training dataset  \n",
    "\n",
    "### Data Quality Check:\n",
    "\n",
    "Before training, verify:\n",
    "- [ ] At least 100 IPOs in dataset\n",
    "- [ ] <20% missing values in key features\n",
    "- [ ] Success rate between 40-70% (balanced)\n",
    "- [ ] No data leakage (features only use pre-IPO data)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review & Clean Data**\n",
    "   - Check for outliers\n",
    "   - Fill missing values\n",
    "   - Validate calculations\n",
    "\n",
    "2. **Train Model**\n",
    "   - Open `research.ipynb`\n",
    "   - Load `data/historical_ipos.csv`\n",
    "   - Train LightGBM classifier\n",
    "   - Target: AUC > 0.70\n",
    "\n",
    "3. **Start Scoring New IPOs**\n",
    "   - Check IPO calendar weekly\n",
    "   - Score upcoming IPOs with model\n",
    "   - Deploy to QuantConnect\n",
    "\n",
    "**Estimated time to live trading:** 1-2 weeks from here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
